# fixed_realcartoon_storydiffusion.py
"""
FIXED RealCartoon StoryDiffusion - Two Character Version
Using the OFFICIAL StoryDiffusion approach + proper step counting
"""

import sys
import os
import torch
import numpy as np
import random
import copy
from PIL import Image, ImageDraw, ImageFont
from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler

# 2-character set-up
ID_LEN = 4  # reference frames per character
CHAR_TOKENS = ["[ALEX]", "[EMMA]"]  # the tags that appear in your prompts
CHAR_DESCS = [
    "a beautiful anime girl with long flowing brown hair, bright blue eyes, soft facial features, wearing a white dress",
    "a handsome anime boy with short black hair, green eyes, gentle expression, wearing a blue casual shirt",
]
# individual reference prompts for each character
REF_PROMPTS = [
    ["facing forward, neutral expression, soft lighting", "gentle smile, looking directly at viewer",
     "side profile, soft glow", "happy cheerful expression, detailed artwork"],
    ["facing forward, neutral expression, soft lighting", "friendly smile, looking directly at viewer",
     "side profile, soft glow", "confident expression, detailed artwork"],
]

# Try to import official StoryDiffusion
try:
    sys.path.insert(0, '.')
    from utils.gradio_utils import SpatialAttnProcessor2_0
    OFFICIAL_STORYDIFFUSION = True
    print("‚úÖ Official StoryDiffusion components found")
except ImportError:
    print("‚ö†Ô∏è Official StoryDiffusion not found - using FIXED custom implementation")
    OFFICIAL_STORYDIFFUSION = False

# FIXED custom processor with proper step counting
class FixedRealCartoonProcessor:
    """
    FIXED StoryDiffusion processor that actually works
    """
    def __init__(self, hidden_size=None, cross_attention_dim=None):
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        self.is_self_attention = cross_attention_dim is None
        self.layer_name = None  # Will be set during registration
        
    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):
        global state
        
        # FIX #1: Proper step counting per layer
        step_key = f"{state.current_character}_{self.layer_name}_{state.denoising_step}"
        
        # Reference mode: Store features with proper indexing
        if state.is_reference_mode and state.current_character and self.is_self_attention:
            if step_key not in state.reference_features:
                # Store high-quality features for this specific layer and step
                stored = hidden_states[0:1].clone().detach()
                state.reference_features[step_key] = stored
                # print(f"üìù Stored: {step_key} shape: {stored.shape}")
        
        # Story mode: Apply consistency with proper indexing
        elif not state.is_reference_mode and state.current_character and self.is_self_attention:
            if step_key in state.reference_features:
                try:
                    stored = state.reference_features[step_key].to(hidden_states.device, dtype=hidden_states.dtype)
                    
                    # Only blend if shapes match exactly (no forced reshaping)
                    if stored.shape == hidden_states[0:1].shape:
                        batch_size = hidden_states.shape[0]
                        if batch_size > 1:
                            stored = stored.repeat(batch_size, 1, 1)
                        
                        # Strong consistency for cartoon characters
                        strength = state.consistency_strength
                        hidden_states = hidden_states * (1.0 - strength) + stored * strength
                        # print(f"‚úÖ Blended: {step_key} strength: {strength}")
                    else:
                        pass  # Skip if shapes don't match exactly
                        # print(f"‚ö†Ô∏è Shape mismatch: {step_key} stored: {stored.shape} vs current: {hidden_states[0:1].shape}")
                        
                except Exception as e:
                    pass  # Continue with normal processing
                    # print(f"‚ùå Blend failed: {step_key} - {e}")
        
        return self._standard_attention(attn, hidden_states, encoder_hidden_states, attention_mask, temb)
    
    def _standard_attention(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):
        """Standard attention processing"""
        residual = hidden_states

        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)

        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)

        batch_size, sequence_length, channel = hidden_states.shape

        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)

        query = attn.to_q(hidden_states)

        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states

        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads

        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)

        hidden_states = torch.nn.functional.scaled_dot_product_attention(
            query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False
        )

        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states.to(query.dtype)

        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)

        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)

        if attn.residual_connection:
            hidden_states = hidden_states + residual

        hidden_states = hidden_states / attn.rescale_output_factor

        return hidden_states

# FIXED global state with proper step tracking
class FixedStoryState:
    def __init__(self):
        self.reference_features = {}
        self.current_character = None
        self.is_reference_mode = True
        self.denoising_step = 0
        self.consistency_strength = 0.8  # Strong consistency for cartoons

    def reset_for_new_generation(self):
        """Reset for a new image generation"""
        self.denoising_step = 0

    def increment_step(self):
        """Increment denoising step"""
        self.denoising_step += 1

state = FixedStoryState()

def setup_seed(seed):
    """Setup random seeds"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True

def setup_realcartoon_pipeline(model_path, device="cuda"):
    """Setup pipeline optimized for RealCartoon XL v7"""
    print("üé® FIXED REALCARTOON XL v7 STORYDIFFUSION - TWO CHARACTERS")
    print("üîß Proper step counting + ALL self-attention layers")
    print("üéØ Consistent Alex and Emma!")
    print("=" * 60)
    
    # Load RealCartoon XL v7
    pipe = StableDiffusionXLPipeline.from_single_file(
        model_path,
        torch_dtype=torch.float16,
        use_safetensors=True
    ).to(device)
    
    # Optimize scheduler for RealCartoon
    pipe.scheduler = DPMSolverMultistepScheduler.from_config(
        pipe.scheduler.config,
        use_karras_sigmas=True,
        algorithm_type="dpmsolver++",
        solver_order=2
    )
    
    # Memory optimizations
    pipe.enable_vae_slicing()
    pipe.enable_model_cpu_offload()
    pipe.enable_attention_slicing(1)
    
    try:
        pipe.enable_xformers_memory_efficient_attention()
        print("‚úÖ xFormers enabled")
    except:
        print("‚ö†Ô∏è xFormers not available")
    
    return pipe

def setup_fixed_storydiffusion(pipe):
    """
    Attach TWO SpatialAttnProcessor2_0 objects to every self-attention layer:
    slot 0 ‚Üí first character, slot ID_LEN ‚Üí second character.
    """
    if OFFICIAL_STORYDIFFUSION:
        print("üîå Using OFFICIAL StoryDiffusion (recommended)")
        
        # Sync global variables with official implementation
        import utils.gradio_utils as gradio_utils
        gradio_utils.total_count = 0
        gradio_utils.attn_count = 0
        gradio_utils.cur_step = 0
        gradio_utils.write = True  # Start in reference mode
        gradio_utils.height = 1024
        gradio_utils.width = 1024
        gradio_utils.sa32 = 0.8
        gradio_utils.sa64 = 0.8
        gradio_utils.sa16 = 0.8
        gradio_utils.mask1024 = None
        gradio_utils.mask4096 = None
        
        # Apply official processors with two-character setup
        attn_procs = {}
        p0_list, p1_list = [], []

        for name in pipe.unet.attn_processors.keys():
            cross_dim = None if name.endswith("attn1.processor") else pipe.unet.config.cross_attention_dim

            # (1) self-attention ‚ûú add two character processors
            if cross_dim is None:
                p0 = SpatialAttnProcessor2_0(id_length=ID_LEN, n_characters=2,
                                             slot_offset=0, layer_tag=name)
                p1 = SpatialAttnProcessor2_0(id_length=ID_LEN, n_characters=2,
                                             slot_offset=ID_LEN, layer_tag=name)
                attn_procs[name] = p0  # UNet stores only one object
                p0_list.append(p0)  # flat lists for later
                p1_list.append(p1)
            else:
                attn_procs[name] = pipe.unet.attn_processors[name]  # keep cross-attn unchanged

        pipe.unet.set_attn_processor(copy.deepcopy(attn_procs))
        print(f"‚úÖ two-slot StoryDiffusion attached to {len(p0_list)} self-attention layers")
        return pipe, p0_list, p1_list
        
    else:
        print("üîß Using FIXED custom implementation")
        
        # FIX #2: Apply to ALL self-attention layers (not just up-blocks)
        processor_count = 0
        attn_procs = {}
        
        for name in pipe.unet.attn_processors.keys():
            cross_attention_dim = (
                None if name.endswith("attn1.processor") 
                else pipe.unet.config.cross_attention_dim
            )
            
            # FIXED: Apply to ALL self-attention layers
            if cross_attention_dim is None:
                processor = FixedRealCartoonProcessor(
                    hidden_size=pipe.unet.config.block_out_channels[-1],
                    cross_attention_dim=cross_attention_dim
                )
                processor.layer_name = name  # Store layer name for indexing
                attn_procs[name] = processor
                processor_count += 1
            else:
                attn_procs[name] = pipe.unet.attn_processors[name]
        
        pipe.unet.set_attn_processor(copy.deepcopy(attn_procs))
        print(f"‚úÖ Applied FIXED processors to {processor_count} self-attention layers")
        return pipe, None, None

def set_storydiffusion_mode(is_reference=True):
    """Set StoryDiffusion mode with proper global sync"""
    global state
    
    if OFFICIAL_STORYDIFFUSION:
        import utils.gradio_utils as gradio_utils
        gradio_utils.write = is_reference
        gradio_utils.cur_step = 0
        gradio_utils.attn_count = 0
    
    state.is_reference_mode = is_reference
    state.reset_for_new_generation()
    
    mode = "REFERENCE" if is_reference else "STORY"
    print(f"üîÑ Mode: {mode}")

# Hook to increment step counter during generation
original_unet_forward = None

def patched_unet_forward(self, *args, **kwargs):
    """Patched UNet forward to increment step counter"""
    global state
    if not OFFICIAL_STORYDIFFUSION:
        state.increment_step()
    result = original_unet_forward(*args, **kwargs)
    return result

def feed_references(pipe, proc_list, prompts, seed_base):
    """
    Store ID_LEN reference images for one character into its slot.
    """
    if not OFFICIAL_STORYDIFFUSION or not proc_list:
        return
        
    # Enable write mode for processors
    for p in proc_list:
        p.write = True

    for i, sub_prompt in enumerate(prompts):
        s = seed_base + i * 123
        setup_seed(s)
        _ = pipe(
            prompt=sub_prompt,
            negative_prompt="",  # keep simple
            num_inference_steps=25,
            generator=torch.Generator(pipe.device).manual_seed(s)
        )
        print(f"‚úÖ Reference {i+1} done")

    # Disable write mode to prevent accidental overwrites
    for p in proc_list:
        p.write = False

def generate_fixed_references(pipe, character_desc, reference_prompts, seed=12345):
    """Generate reference images with FIXED StoryDiffusion"""
    global state, original_unet_forward
    
    print(f"\nüé¨ REFERENCE GENERATION")
    set_storydiffusion_mode(is_reference=True)
    state.current_character = "main_character"
    
    # Patch UNet for step counting (if using custom implementation)
    if not OFFICIAL_STORYDIFFUSION and original_unet_forward is None:
        original_unet_forward = pipe.unet.forward
        pipe.unet.forward = patched_unet_forward.__get__(pipe.unet, pipe.unet.__class__)
    
    reference_images = []
    
    for i, prompt in enumerate(reference_prompts):
        state.reset_for_new_generation()
        
        # Setup seed
        ref_seed = seed + i * 111
        setup_seed(ref_seed)
        generator = torch.Generator(device=pipe.device).manual_seed(ref_seed)
        
        # Enhanced prompt for RealCartoon
        enhanced_prompt = f"masterpiece, best quality, anime portrait of {character_desc}, {prompt}, beautiful anime art style, detailed cartoon illustration, vibrant colors"
        negative_prompt = "photorealistic, realistic, 3d render, blurry, low quality, bad anatomy, bad hands, missing fingers, extra fingers, mutated hands, deformed, ugly, bad proportions, extra limbs, malformed limbs, bad eyes, cross-eyed, different eyes, inconsistent character, multiple people, different person"
        
        print(f"  üé® Reference {i+1}: {prompt[:50]}...")
        
        image = pipe(
            prompt=enhanced_prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=30,
            guidance_scale=8.0,
            height=1024,
            width=1024,
            generator=generator,
        ).images[0]
        
        reference_images.append(image)
        print(f"  ‚úÖ Generated reference {i+1}")
    
    print(f"‚úÖ Generated {len(reference_images)} reference images")
    return reference_images

def generate_story_images(pipe, story_prompts, seed=22345):
    """Generate story images with two characters"""
    global state
    
    print(f"\nüé¨ STORY GENERATION - TWO CHARACTERS")
    set_storydiffusion_mode(is_reference=False)
    
    story_images = []
    
    for i, prompt in enumerate(story_prompts):
        state.reset_for_new_generation()
        
        # Closer seeds for consistency
        story_seed = seed + i * 50
        setup_seed(story_seed)
        generator = torch.Generator(device=pipe.device).manual_seed(story_seed)
        
        # Enhanced prompt for two character consistency
        enhanced_prompt = f"masterpiece, best quality, anime illustration, {prompt}, beautiful anime art style, detailed cartoon illustration, vibrant colors, character consistency"
        negative_prompt = "photorealistic, realistic, 3d render, blurry, low quality, bad anatomy, bad hands, missing fingers, extra fingers, mutated hands, deformed, ugly, bad proportions, extra limbs, malformed limbs, bad eyes, cross-eyed, different eyes, inconsistent character, changing face, inconsistent art style"
        
        print(f"  üé® Story {i+1}: {prompt[:50]}...")
        
        image = pipe(
            prompt=enhanced_prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=25,
            guidance_scale=7.5,
            height=1024,
            width=1024,
            generator=generator,
        ).images[0]
        
        story_images.append(image)
        print(f"  ‚úÖ Generated story {i+1}")
    
    print(f"‚úÖ Generated {len(story_images)} story images")
    return story_images

def create_comparison_grid(reference_images_alex, reference_images_emma, story_images):
    """Create comparison grid with both characters"""
    # Calculate grid dimensions
    ref_count = len(reference_images_alex)
    story_count = len(story_images)
    cols = max(ref_count, story_count)
    img_size = 400
    margin = 10
    
    grid_width = cols * img_size + (cols - 1) * margin
    grid_height = 3 * img_size + 2 * margin  # 3 rows: Alex refs, Emma refs, Story
    
    grid = Image.new('RGB', (grid_width, grid_height), 'white')
    
    # Row 1: Alex references
    for i, img in enumerate(reference_images_alex):
        x = i * (img_size + margin)
        resized = img.resize((img_size, img_size), Image.Resampling.LANCZOS)
        grid.paste(resized, (x, 0))
    
    # Row 2: Emma references
    for i, img in enumerate(reference_images_emma):
        x = i * (img_size + margin)
        resized = img.resize((img_size, img_size), Image.Resampling.LANCZOS)
        grid.paste(resized, (x, img_size + margin))
    
    # Row 3: Story images
    for i, img in enumerate(story_images):
        x = i * (img_size + margin)
        resized = img.resize((img_size, img_size), Image.Resampling.LANCZOS)
        grid.paste(resized, (x, 2 * (img_size + margin)))
    
    return grid

def main():
    """Test the FIXED StoryDiffusion with two characters"""
    
    # Find model
    possible_paths = [
        "../models/realcartoonxl_v7.safetensors",
        "../../models/realcartoonxl_v7.safetensors",
        "../realcartoonxl_v7.safetensors",
        "models/realcartoonxl_v7.safetensors"
    ]
    
    model_path = None
    for path in possible_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if not model_path:
        print("‚ùå RealCartoon XL v7 model not found!")
        return
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    try:
        # Setup pipeline
        pipe = setup_realcartoon_pipeline(model_path, device)
        
        # Setup FIXED StoryDiffusion
        result = setup_fixed_storydiffusion(pipe)
        if OFFICIAL_STORYDIFFUSION:
            pipe, p0_list, p1_list = result
        else:
            pipe = result
            p0_list, p1_list = None, None
        
        # Generate references for both characters
        print("\n" + "="*60)
        print("GENERATING CHARACTER REFERENCES")
        print("="*60)
        
        alex_refs = []
        emma_refs = []
        
        if OFFICIAL_STORYDIFFUSION and p0_list and p1_list:
            # Feed references for each character
            for idx, (token, desc, ref_list) in enumerate(zip(CHAR_TOKENS, CHAR_DESCS, REF_PROMPTS)):
                print(f"\nüé¨ Processing reference set for {token}")
                proc_list = p0_list if idx == 0 else p1_list  # Choose the right family
                full_prompts = [
                    f"masterpiece, best quality, {desc}, {token}, {pose}"
                    for pose in ref_list
                ]
                feed_references(pipe, proc_list, full_prompts, 1000 + idx*1000)
                
                # Also generate reference images for display
                refs = generate_fixed_references(pipe, desc, ref_list, seed=1000 + idx*1000)
                if idx == 0:
                    alex_refs = refs
                else:
                    emma_refs = refs

            # After both passes, lock processors in read-only mode
            for p in p0_list + p1_list:
                p.write = False
        else:
            # Generate references normally for display
            alex_refs = generate_fixed_references(pipe, CHAR_DESCS[0], REF_PROMPTS[0], seed=1000)
            emma_refs = generate_fixed_references(pipe, CHAR_DESCS[1], REF_PROMPTS[1], seed=2000)
        
        # Story prompts with both characters - SIMPLE BACKGROUNDS
        STORY_PROMPTS = [
            "[ALEX] and [EMMA] sitting close together, simple white background",
            "[ALEX] showing [EMMA] something, both looking at each other, neutral background", 
            "[ALEX] and [EMMA] standing side by side, soft lighting, plain background",
            "[EMMA] laughing while [ALEX] smiles, simple studio lighting, clean background",
            "[ALEX] and [EMMA] holding hands, facing viewer, minimalist background",
            "[ALEX] and [EMMA] looking at each other warmly, soft lighting, empty background",
            "[ALEX] giving [EMMA] a gentle hug, simple lighting, plain background",
            "[ALEX] and [EMMA] standing together happily, studio portrait style, clean background"
        ]
        
        # Generate story images
        print("\n" + "="*60)
        print("GENERATING STORY SEQUENCE")
        print("="*60)
        
        story_images = generate_story_images(pipe, STORY_PROMPTS, seed=4000)
        
        # Save results
        output_dir = "two_character_story_output"
        os.makedirs(output_dir, exist_ok=True)
        
        # Save individual images
        for i, img in enumerate(alex_refs):
            img.save(f"{output_dir}/alex_ref_{i+1:02d}.png")
        
        for i, img in enumerate(emma_refs):
            img.save(f"{output_dir}/emma_ref_{i+1:02d}.png")
            
        for i, img in enumerate(story_images):
            img.save(f"{output_dir}/story_{i+1:02d}.png")
        
        # Create comparison grid
        grid = create_comparison_grid(alex_refs, emma_refs, story_images)
        grid.save(f"{output_dir}/two_character_comparison.png")
        
        print(f"\nüéâ TWO CHARACTER STORYDIFFUSION COMPLETE!")
        print(f"‚úÖ Generated {len(alex_refs)} Alex references")
        print(f"‚úÖ Generated {len(emma_refs)} Emma references") 
        print(f"‚úÖ Generated {len(story_images)} story images with both characters")
        print(f"üìÅ Saved to: {output_dir}/")
        print(f"\nüë• SIMPLE STORY SEQUENCE:")
        print(f"  1. Sitting close together")
        print(f"  2. Showing something to each other")
        print(f"  3. Standing side by side")
        print(f"  4. Sharing laughs")
        print(f"  5. Holding hands")
        print(f"  6. Looking at each other warmly")
        print(f"  7. Gentle hug")
        print(f"  8. Standing together happily")
        print(f"\nüîß FIXES APPLIED:")
        print(f"  ‚úÖ Two-character separation with distinct processor slots")
        print(f"  ‚úÖ {'Official' if OFFICIAL_STORYDIFFUSION else 'Fixed custom'} implementation")
        print(f"  ‚úÖ Literal [ALEX] and [EMMA] tags in prompts")
        print(f"\nüí° Story frames with ALEX and EMMA should now be PERFECT!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()